{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Algorithms",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY0QtbdgnzdZ"
      },
      "source": [
        "**Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_xIbREC7nP0"
      },
      "source": [
        "\n",
        "Linear regression performs the task to predict the variable y from the given independent variable x. So this technique finds out the linear relationship between x and y.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJSaIqjn4lM"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "height=[[4.0],[5.0],[6.0],[7.0],[8.0],[9.0],[10.0]]\n",
        "weight=[  8, 10 , 12, 14, 16, 18, 20]\n",
        "plt.scatter(height,weight,color='black')\n",
        "plt.xlabel(\"height\")\n",
        "plt.ylabel(\"weight\")\n",
        "reg=linear_model.LinearRegression()\n",
        "reg.fit(height,weight)\n",
        "X_height=[[12.0]]\n",
        "print(reg.predict(X_height))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQSXpWkvn7X7"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLnXOvqm8sR7"
      },
      "source": [
        "\n",
        "\n",
        "*   \n",
        "Logistic regression is used for predicting the categorical dependent variable using a given set of independent variables.\n",
        "\n",
        "*   Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulx7WlMyn-1f"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "X = [[30],[40],[50],[60],[20],[10],[70]]\n",
        "y = [0,1,1,1,0,0,1]\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X,y)\n",
        "X_marks=[[20]]\n",
        "print(classifier.predict(X_marks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7edYM0soAf3"
      },
      "source": [
        "**Decision Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYAZ8kWq9SxT"
      },
      "source": [
        "\n",
        "\n",
        "*   Decision tree can be used to\n",
        "classification and regression both having a tree like structure.\n",
        "*   \n",
        "In this the best attribute is placed at the root node and is split into 2 subsets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ9MrYQNoISn"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "X = [[30],[40],[50],[60],[20],[10],[70]]\n",
        "y = [0,1,1,1,0,0,1]\n",
        "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X,y)\n",
        "X_marks=[[20]]\n",
        "print(classifier.predict(X_marks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmrrEq1aoS-Q"
      },
      "source": [
        "**K Nearest Neighbours**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uHUVR4m-TBg"
      },
      "source": [
        " The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. Can be used to solve both classification and regression problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1lqrJtjoTrh"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "X = [[30],[40],[50],[60],[20],[10],[70]]\n",
        "y = [0,1,1,1,0,0,1]\n",
        "from sklearn.neighbors import KNeighborsClassifier  \n",
        "classifier= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )  \n",
        "classifier.fit(X,y) \n",
        "X_marks=[[50]]\n",
        "print(classifier.predict(X_marks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03CiqNs2oVHy"
      },
      "source": [
        "**Support Vector Machine**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu7PtyKa-d6E"
      },
      "source": [
        "The main goal of SVM is to divide the class into 2 datasets to find the maximum marginal hyperplanes.\n",
        "\n",
        "\n",
        "*  Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFcvVdhAoXtB"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "X = [[30],[40],[50],[60],[20],[10],[70]]\n",
        "y = [0,1,1,1,0,0,1]\n",
        "classifier = SVC(kernel = 'linear', random_state = 0)\n",
        "classifier.fit(X,y)\n",
        "X_marks=[[55]]\n",
        "print(classifier.predict(X_marks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l4iEDfIoZew"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqC_u9co_JCl"
      },
      "source": [
        "Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. \n",
        "* Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our modelâ€™s prediction.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXeAV2LDobzc"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "X = [[30],[40],[50],[60],[20],[10],[70]]\n",
        "y = [0,1,1,1,0,0,1]\n",
        "RandomForestRegModel = RandomForestRegressor()\n",
        "RandomForestRegModel.fit(X,y)\n",
        "classifier.fit(X,y)\n",
        "X_marks=[[70]]\n",
        "print(RandomForestRegModel.predict(X_marks))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}